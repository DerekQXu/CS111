NAME: Derek Xu
EMAIL: derekqxu@g.ucla.edu
ID: 704751588

Description of included files:
SortedList.h - provided header file for SortedList that compiles cleanly with no errors
SortedList.c - implementation of SortedList that compiles cleanly with no errors
README - description of files and answers to questions
Makefile - Compiles and runs the project; supports following options:
	build, tests, graphs, dist, clean	
lab2_list-X.png - gnuplots of the lab2_list.c test results
lab2_list.gp - provided gnuplot implementation for lab2_list.c
lab2_list.csv - generated output for lab2_list.c test results
lab2_add-X.png - gnuplots of the lab2_add.c test results
lab2_add.gp - provided gnuplot implementation for lab2_add.c
lab2_add.csv - generated output for lab2_add.c test results
lab2_list.c - C source module for lab2 list that compiles cleanly with no errors
	options: threads iterations yield(idl) sync(m,s)
lab2_add.c - C source module for lab2 list that compiles cleanly with no errors
	options: threads iterations yield sync(m,s,c)

Questions:
2.1.1
For small iterations, the thread will complete faster than it gets created. This is because
the pthread library accesses system calls to create the thread. At very small iterations, the
first thread will finish before the next thread is created, thus there will be no race
conditions, as the threads cannot interrupt eachother. At small iterations, the program will
seldom fail because the time that both threads are running is less, thus the probability of
the threads stepping on eachother is less.

2.1.2
--yield runs slower because of context switching, which has overhead and reduces the amount
of true parallelism in the program. The additional time is going to the context switch. It is
not possible to get valid per-operation timing with the --yield option:
	1. the time of context switch is more than that of the add() command
	2. the linux server runs the threads truely in parallel, thus any time we record will
	   be more than the actual running time
	3. the add() command runs so fast that recording the time for each add() (not
	   add_func()) will be more precise than the clock's gettime.

2.1.3
The average cost per operation drops because we start our clock before thread initialization,
thus we incur additional overhead from the pthread creation time. This time becomes less
impactful on our total running time as we increase the number of iterations we run. We can
find the correct cost by running many threads and finding the value that the cost per
operation asymptotes to when the iteration approaches infinity. i.e. we can choose a number
of iterations where the curve of iteration to cost per operation flatlines.

2.1.4
All of the options perform similarly for low number of threads there is less contention. When
there is less contention the additional time used for locks and the system calls in the lock
implementation is reduced, thus they all being to perform similarly. The protected operations
slows down as thread number increases because there will be more system calls to ensure that
the locking is implemented correctly. Furthermore, spin locks have a hardware requirement that
forces the data in the L3/L2 caches to be overwritten, which increases the overall time it
takes to perform these protected operations. 

2.2.1
In the list case, the curve is increasing and relatively linearly. In the add case, the curve
is increasing and asymptotes to some value. The list cost increases faster than the add case.
Both are increasing, because as we have more threads, we have more contention. The shapes and
amount of increase is different because as we increase the number of threads and keep the
number of iterations the same in the add case, we are still performing the same number of
operations... in the list case, as we increase the number of threads, we perform more
operations, as each thread is still performing the same number of operations. As was said in
lecture, we increase contention, by increasing the amount of time that we spend in the
critical section and we increase the number of threads competing for the lock, and this is
what is happening here. Note, when we say linear, we mean linear in the logarithmic scale
shown on the gnuplot.

2.2.2
In the mutex case, the curve is increasing and relatively linear. In the spin lock case, the
curve is increasing and relatively linear. The mutex lock is less expensive than the spin
lock as we increase the number of operations. In software, we learn that this is due to spin
locks spinning... that is we are wasting CPU cycles as we are waiting for the lock. But there
is a more legitimate answer regarding hardware. As mentioned in discussion, the spin lock
would change the L3 cache and update it with its own value. The L3 cache is shared among
various different threads, thus when another thread is executing it cannot take advantage of
its L3 cache (as it likely accesses different locations)... this additional cost makes the
spin lock significantly slower and scale worse with threads, compared to mutexes. Note, when
we say linear, we mean linear in the logarithmic scale shown on the gnuplot.

References:
previous projects
lectures
discussion
project specifications
man pages of functions on the spec sheet
https://stackoverflow.com/questions/1383363/is-my-spin-lock-implementation-correct-and-optimal
https://www.tutorialspoint.com/c_standard_library/c_function_strcat.htm
https://stackoverflow.com/questions/23250863/difference-between-pthread-and-lpthread-while-compiling
